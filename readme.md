# Evaluating Word Embeddings in Fictional Narratives

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/) [![Jupyter Notebook](https://img.shields.io/badge/Jupyter_Notebook-%F0%9F%93%96-orange)](https://jupyter.org/)  

### A Comparative Study of Word2Vec, FastText, and GloVe in Literary Text Analysis

This repository provides a **Jupyter Notebook implementation** of a research study comparing **Word2Vec, FastText, and GloVe** for analyzing **fictional narratives**. The code processes text from **epic, fantasy, and detective fiction** to examine how these word embeddings handle **semantic relationships, analogy tasks, and word clustering**.

---

## Features
**Preprocesses Fictional Text** â€“ Tokenization, stopword removal, lemmatization  
**Trains Word2Vec & FastText Models** â€“ Captures semantic meaning from text  
**Loads Pretrained GloVe Vectors** â€“ Uses Stanford's 100D embeddings  
**Evaluates Cosine Similarity** â€“ Measures semantic closeness of words  
**Performs Analogy Tasks** â€“ Tests conceptual relationships between words  
**Visualizes Embeddings (PCA/t-SNE)** â€“ Shows word clusters in 2D space  
**Computes Clustering Quality (Silhouette Score)** â€“ Evaluates embedding effectiveness  

---

## Repository Structure

```bash
ğŸ“‚ word-embeddings-fiction
â”‚â”€â”€ ğŸ“œ README.md            # Documentation for the repository
â”‚â”€â”€ ğŸ“œ requirements.txt      # List of required Python dependencies
â”‚â”€â”€ ğŸ“œ word_embeddings.ipynb # Jupyter Notebook with full implementation
â”‚â”€â”€ ğŸ“œ glove.6B.100d.txt     # Pretrained GloVe embeddings (downloaded)
â”‚â”€â”€ ğŸ“œ word2vec_fiction.model # Saved Word2Vec model
â”‚â”€â”€ ğŸ“œ fasttext_fiction.model # Saved FastText model
```
## Installation & Setup

### **Clone the Repository**
Clone the GitHub repository to your local system:
```bash
git clone https://github.com/vrundag91/Evaluation-of-the-performance-of-word-embeddings-in-fictional-narratives.git
cd word-embeddings-fiction
```
### **Create a Virtual Environment (Optional but Recommended)**
```bash
python3 -m venv env
source env/bin/activate  # For Mac/Linux
env\Scripts\activate     # For Windows
```
### **Install Required Dependencies**
```bash
pip install -r requirements.txt
```
### **Download & Extract Pretrained GloVe Embeddings**
Run the following command inside a Python script or Jupyter Notebook:
```bash
import urllib.request
import zipfile

glove_url = "http://nlp.stanford.edu/data/glove.6B.zip"
urllib.request.urlretrieve(glove_url, "glove.6B.zip")

with zipfile.ZipFile("glove.6B.zip", 'r') as zip_ref:
    zip_ref.extractall("glove_data")

print("GloVe embeddings downloaded and extracted!")

```

## ğŸ“Š Key Findings

1ï¸. **FastText performs best for fictional text** as it handles **rare words & invented vocabulary** (e.g., *"Muggle"* in Harry Potter).  
2ï¸. **Word2Vec is effective for structured texts** but struggles with **out-of-vocabulary (OOV) words**.  
3ï¸. **GloVe performs well on general NLP tasks** but lacks **genre-specific adaptability** for fiction.  
4ï¸. **FastText produces better-defined clusters** in **t-SNE and PCA visualizations**.  
5ï¸. **Silhouette Score confirms FastText forms stronger semantic clusters** compared to Word2Vec & GloVe.  


